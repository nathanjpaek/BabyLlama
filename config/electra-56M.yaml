data:
  tokenizer_path: "./models/gpt-clean-16000-gutenberg-BETTER.json"
  train_path: "./data/gutenberg_10M"
  eval_path: "./data/babylm_dev_clean"
  seq_length: 128
  eval_samples: 8192

model:
  type: "Electra"
  name: "Electra-50M"
  hidden_size: 768  # Increased from 640
  intermediate_size: 3072  # 4 * hidden_size for Electra
  n_layer: 6  # Keeping it at 6 layers
  n_head: 8  # Keeping attention heads at 8
  resid_pdrop: 0.1
  embd_pdrop: 0.1
  attn_pdrop: 0.1

training:
  lr: 2.5e-4
  batch_size: 128
  num_epochs: 4
  gradient_accumulation_steps: 16
  warmup_steps: 1000
  fp16: True

logging:
  wandb: True
  project: "babylm-dev"
  output_dir: "./models/"
